{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BitsAndBytesConfig\n",
    "from tqdm import tqdm \n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device: \" ,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'data/arena'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(root,\"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(root,\"test.csv\"))\n",
    "sample_submission_df = pd.read_csv(os.path.join(root,\"sample_submission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (57477, 9)\n",
      "test shape: (3, 4)\n",
      "------------------------------------------------------------------------------------------\n",
      "train missing values: 0\n",
      "test missing values: 0\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "train_df['prompt'] = train_df['prompt'].apply(process)\n",
    "train_df['response_a'] = train_df['response_a'].apply(process)\n",
    "train_df['response_b'] = train_df['response_b'].apply(process)\n",
    "\n",
    "test_df['prompt'] = test_df['prompt'].apply(process)\n",
    "test_df['response_a'] = test_df['response_a'].apply(process)\n",
    "test_df['response_b'] = test_df['response_b'].apply(process)\n",
    "\n",
    "\n",
    "print(f\"train shape: {train_df.shape}\")\n",
    "print(f\"test shape: {test_df.shape}\")\n",
    "print(\"-\"*90)\n",
    "print(f\"train missing values: {train_df.isnull().sum().sum()}\")\n",
    "print(f\"test missing values: {test_df.isnull().sum().sum()}\")\n",
    "print(\"-\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            max_length  median_length   std_length\n",
      "prompt         33044.0           92.0  1073.120265\n",
      "response_a     53976.0         1072.0  1512.843807\n",
      "response_b     53764.0         1082.0  1536.733530\n"
     ]
    }
   ],
   "source": [
    "def lengths(column):\n",
    "    return column.str.len()\n",
    "\n",
    "# Calculate lengths of the entries in each column\n",
    "train_df['prompt_length'] = lengths(train_df['prompt'])\n",
    "train_df['response_a_length'] = lengths(train_df['response_a'])\n",
    "train_df['response_b_length'] = lengths(train_df['response_b'])\n",
    "\n",
    "# Aggregate statistics\n",
    "statistics = {\n",
    "    'prompt': {\n",
    "        'max_length': train_df['prompt_length'].max(),\n",
    "        'median_length': train_df['prompt_length'].median(),\n",
    "        'std_length': train_df['prompt_length'].std()\n",
    "    },\n",
    "    'response_a': {\n",
    "        'max_length': train_df['response_a_length'].max(),\n",
    "        'median_length': train_df['response_a_length'].median(),\n",
    "        'std_length': train_df['response_a_length'].std()\n",
    "    },\n",
    "    'response_b': {\n",
    "        'max_length': train_df['response_b_length'].max(),\n",
    "        'median_length': train_df['response_b_length'].median(),\n",
    "        'std_length': train_df['response_b_length'].std()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert the statistics dictionary to a DataFrame for better readability\n",
    "statistics_df = pd.DataFrame(statistics).T\n",
    "\n",
    "print(statistics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "57472    0\n",
       "57473    0\n",
       "57474    0\n",
       "57475    0\n",
       "57476    0\n",
       "Name: winner_tie, Length: 57477, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['winner_tie']\n",
    "# train_df['winner_a_binary'] = train_df['winner_model_a'].apply(lambda x : 1 if x == 1 else 0)\n",
    "# train_df['winner_a_binary'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[['prompt','response_a','winner_model_a']][:25000]#40000\n",
    "train_data,val_data = train_test_split(train,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColBERTDataset(Dataset):\n",
    "    def __init__(self,data,tokenizer,max_len=max_tokens):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len =max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        row = self.data.iloc[index]\n",
    "        \n",
    "        prompt = row['prompt']\n",
    "        response_a = row['response_a']\n",
    "        # response_b = row['response_b']\n",
    "        winner_a = row['winner_model_a']\n",
    "        # winner_b = row['winner_model_b']\n",
    "        # winner_tie = row['winner_tie']\n",
    "        \n",
    "        \n",
    "        prompt_tokenized = self.tokenizer(prompt,truncation=True,padding='max_length',max_length = self.max_len,return_tensors='pt')\n",
    "        response_a_tokenized = self.tokenizer(response_a,truncation=True,padding='max_length',max_length = self.max_len,return_tensors='pt')\n",
    "        # response_b_tokenized = self.tokenizer(response_b,truncation=True,padding='max_length',max_length = self.max_len,return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'prompt_inputs' :prompt_tokenized['input_ids'].squeeze(),\n",
    "            'prompt_attention_mask': prompt_tokenized['attention_mask'].squeeze(),\n",
    "            \n",
    "            'response_a_inputs' :response_a_tokenized['input_ids'].squeeze(),\n",
    "            'response_a_attention_mask': response_a_tokenized['attention_mask'].squeeze(),\n",
    "            \n",
    "            # 'response_b_inputs' :response_b_tokenized['input_ids'].squeeze(),\n",
    "            # 'response_b_attention_mask': response_b_tokenized['attention_mask'].squeeze(),\n",
    "            \n",
    "            'winner_model_a': torch.tensor(winner_a, dtype=torch.float),\n",
    "            # 'winner_model_b': torch.tensor(winner_b, dtype=torch.float),\n",
    "            # 'winner_tie': torch.tensor(winner_tie, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smart/sanket/experiments/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "# Create dataset and dataloaders\n",
    "train_dataset = ColBERTDataset(train, tokenizer)\n",
    "val_dataset = ColBERTDataset(val_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_loader:\n",
    "#     print(i)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt_inputs', 'prompt_attention_mask', 'response_a_inputs', 'response_a_attention_mask', 'winner_model_a'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResponseClassifier(nn.Module):\n",
    "#     def __init__(self,model_name,hidden_size,intermediate_size,num_classes,num_heads=8):\n",
    "#         super(ResponseClassifier,self).__init__()\n",
    "#         self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "#         self.attention = nn.MultiheadAttention(embed_dim=hidden_size,\n",
    "#                                                num_heads=num_heads,\n",
    "#                                                batch_first=True)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(hidden_size*2,intermediate_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(intermediate_size,num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self,prompt_embeddings,response_embeddings):\n",
    "#         combined_embeddings = torch.cat((prompt_embeddings, response_embeddings), dim=1)\n",
    "        \n",
    "#         attended_output, _ = self.attention(combined_embeddings, combined_embeddings, combined_embeddings)\n",
    "        \n",
    "#         pooled_output = torch.mean(attended_output, dim=1)\n",
    "        \n",
    "#         logits = self.classifier(pooled_output)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseClassifier(nn.Module):\n",
    "    def __init__(self, model_name, hidden_size, intermediate_size, num_classes, num_heads=8):\n",
    "        super(ResponseClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding_model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        self.attention_layer_a = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True, dropout=0.2)\n",
    "        # self.attention_layer_b = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(intermediate_size, num_classes)\n",
    "        )\n",
    "            \n",
    "    def forward(self, prompt_embeddings, response_a_embeddings):\n",
    "        # Attention on prompt and response_a\n",
    "        combined_embeddings_a = torch.cat((prompt_embeddings, response_a_embeddings), dim=1)\n",
    "        attended_output_a, _ = self.attention_layer_a(combined_embeddings_a, combined_embeddings_a, combined_embeddings_a)\n",
    "        \n",
    "        # Attention on prompt and response_b\n",
    "        # combined_embeddings_b = torch.cat((prompt_embeddings, response_b_embeddings), dim=1)\n",
    "        # attended_output_b, _ = self.attention_layer_b(combined_embeddings_b, combined_embeddings_b, combined_embeddings_b)\n",
    "        \n",
    "        # Pooling the attended outputs\n",
    "        pooled_output_a = torch.mean(attended_output_a, dim=1)\n",
    "        # pooled_output_b = torch.mean(attended_output_b, dim=1)\n",
    "        \n",
    "        # Combine pooled outputs from both attentions\n",
    "        # combined_pooled_output = torch.cat((pooled_output_a, pooled_output_b), dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(pooled_output_a)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allenai/longformer-base-4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing code\n",
    "test =  train_df[['prompt','response_a','winner_model_a']][50000:]\n",
    "def get_predictions(test_df, model, tokenizer, device):\n",
    "    model.embedding_model.eval()\n",
    "    model.eval()\n",
    "    \n",
    "    winner_a_predicted = []\n",
    "    # winner_b_predicted = []\n",
    "    # winner_tie_predicted = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, row in test_df.iterrows():\n",
    "            prompt = row['prompt']\n",
    "            response_a = row['response_a']\n",
    "            # response_b = row['response_b']\n",
    "            \n",
    "            prompt_tokenized = tokenizer(prompt, truncation=True, padding='max_length', max_length=max_tokens, return_tensors='pt')\n",
    "            response_a_tokenized = tokenizer(response_a, truncation=True, padding='max_length', max_length=max_tokens, return_tensors='pt')\n",
    "            # response_b_tokenized = tokenizer(response_b, truncation=True, padding='max_length', max_length=max_tokens, return_tensors='pt')\n",
    "\n",
    "            prompt_input_ids = prompt_tokenized['input_ids'].to(device)\n",
    "            prompt_attention_mask = prompt_tokenized['attention_mask'].to(device)\n",
    "\n",
    "            response_a_input_ids = response_a_tokenized['input_ids'].to(device)\n",
    "            response_a_attention_mask = response_a_tokenized['attention_mask'].to(device)\n",
    "\n",
    "            # response_b_input_ids = response_b_tokenized['input_ids'].to(device)\n",
    "            # response_b_attention_mask = response_b_tokenized['attention_mask'].to(device)\n",
    "\n",
    "            \n",
    "            prompt_embeddings = model.embedding_model(prompt_input_ids,prompt_attention_mask).last_hidden_state\n",
    "            response_a_embeddings = model.embedding_model(response_a_input_ids,response_a_attention_mask).last_hidden_state\n",
    "            # response_b_embeddings = model.embedding_model(response_b_input_ids,response_b_attention_mask).last_hidden_state\n",
    "\n",
    "            output = model(prompt_embeddings,response_a_embeddings)\n",
    "            # Extract probabilities\n",
    "            probabilities = torch.softmax(output, dim=1).cpu().numpy().flatten()\n",
    "\n",
    "            winner_a_predicted.append(probabilities[0])\n",
    "            # winner_b_predicted.append(probabilities[1])\n",
    "            # winner_tie_predicted.append(probabilities[2])\n",
    "            \n",
    "    test_df['winner_a_predicted'] = winner_a_predicted\n",
    "    # test_df['winner_b_predicted'] = winner_b_predicted\n",
    "    # test_df['winner_tie_predicted'] = winner_tie_predicted\n",
    "\n",
    "    return test_df\n",
    "\n",
    "def get_accuracy(test):\n",
    "    test['predicted_class'] = (test['winner_a_predicted'] >= 0.5).astype(int)\n",
    "    \n",
    "    true_a = test['winner_model_a'].tolist()\n",
    "    pred_a = test['predicted_class'].tolist()\n",
    "\n",
    "    # Calculate accuracy for response A wins\n",
    "    accuracy_a = accuracy_score(true_a, pred_a)\n",
    "    print(f\"Accuracy for response A wins: {accuracy_a:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# def get_accuracy(test):\n",
    "#     test['predicted_class'] = test[['winner_a_predicted', 'winner_b_predicted', 'winner_tie_predicted']].idxmax(axis=1)\n",
    "#     test['predicted_class'] = test['predicted_class'].map({\n",
    "#         'winner_a_predicted': 'winner_model_a',\n",
    "#         'winner_b_predicted': 'winner_model_b',\n",
    "#         'winner_tie_predicted': 'winner_tie'\n",
    "#     })\n",
    "#     true_a, pred_a = [], []\n",
    "#     true_b, pred_b = [], []\n",
    "#     true_tie, pred_tie = [], []\n",
    "\n",
    "#     # Populate the lists with corresponding values\n",
    "#     for idx, row in test.iterrows():\n",
    "#         if row['winner_model_a'] == 1:\n",
    "#             true_a.append(1)\n",
    "#             pred_a.append(1 if row['predicted_class'] == 'winner_model_a' else 0)\n",
    "#         else:\n",
    "#             true_a.append(0)\n",
    "#             pred_a.append(1 if row['predicted_class'] == 'winner_model_a' else 0)\n",
    "\n",
    "#         if row['winner_model_b'] == 1:\n",
    "#             true_b.append(1)\n",
    "#             pred_b.append(1 if row['predicted_class'] == 'winner_model_b' else 0)\n",
    "#         else:\n",
    "#             true_b.append(0)\n",
    "#             pred_b.append(1 if row['predicted_class'] == 'winner_model_b' else 0)\n",
    "\n",
    "#         if row['winner_tie'] == 1:\n",
    "#             true_tie.append(1)\n",
    "#             pred_tie.append(1 if row['predicted_class'] == 'winner_tie' else 0)\n",
    "#         else:\n",
    "#             true_tie.append(0)\n",
    "#             pred_tie.append(1 if row['predicted_class'] == 'winner_tie' else 0)\n",
    "\n",
    "#     # Calculate accuracy for each class\n",
    "#     accuracy_a = accuracy_score(true_a, pred_a)\n",
    "#     accuracy_b = accuracy_score(true_b, pred_b)\n",
    "#     accuracy_tie = accuracy_score(true_tie, pred_tie)\n",
    "#     print(f\"Accuracy for response A wins: {accuracy_a:.4f}\")\n",
    "#     print(f\"Accuracy for response B wins: {accuracy_b:.4f}\")\n",
    "#     print(f\"Accuracy for tie: {accuracy_tie:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smart/sanket/experiments/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-08-05 18:16:24.319644: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-05 18:16:24.364968: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-05 18:16:25.049806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/smart/sanket/experiments/venv/lib/python3.8/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = 'allenai/longformer-base-4096'\n",
    "hidden_size = 768\n",
    "intermediate_size = 512\n",
    "num_classes = 1\n",
    "learning_rate = 2e-4\n",
    "num_epochs = 4\n",
    "PRINT_FREQ = 500\n",
    "\n",
    "model = ResponseClassifier(model_name, hidden_size, intermediate_size,num_classes ,num_heads=8).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smart/sanket/experiments/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/smart/sanket/experiments/venv/lib/python3.8/site-packages/transformers/modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "/tmp/ipykernel_1257418/925657186.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/smart/sanket/experiments/model_2.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized model\n"
     ]
    }
   ],
   "source": [
    "# model = ResponseClassifier(model_name,hidden_size,intermediate_size,num_classes,num_heads=8).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr = learing_rate)\n",
    "# criterion = nn.BCELoss()\n",
    "# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=30)\n",
    "# try:\n",
    "#     model.load_state_dict(torch.load('/home/smart/sanket/experiments/model_2.pth'))\n",
    "#     print('loaded saved model successfully')\n",
    "# except Exception as e:\n",
    "#     print('initialized model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:  32%|███▏      | 499/1563 [09:59<21:27,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 1   Batch: 500/1563   Train Loss: 0.6452   LR: 2.0e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:  56%|█████▌    | 875/1563 [17:31<13:49,  1.21s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    \n",
    "    for batch_index, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        prompt_input_ids = batch['prompt_inputs'].to(device)\n",
    "        prompt_attention_mask = batch['prompt_attention_mask'].to(device)\n",
    "        response_a_input_ids = batch['response_a_inputs'].to(device)\n",
    "        response_a_attention_mask = batch['response_a_attention_mask'].to(device)\n",
    "        winner_a = batch['winner_model_a'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prompt_embeddings = model.embedding_model(prompt_input_ids, prompt_attention_mask).last_hidden_state\n",
    "            response_a_embeddings = model.embedding_model(response_a_input_ids, response_a_attention_mask).last_hidden_state\n",
    "\n",
    "        logits = model(prompt_embeddings, response_a_embeddings).squeeze(-1)\n",
    "        loss = criterion(logits, winner_a.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        train_loss += loss.item()\n",
    "        steps += 1\n",
    "        if (batch_index + 1) % PRINT_FREQ == 0:\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(\n",
    "                f'  Epoch: {epoch+1}',\n",
    "                f'  Batch: {batch_index + 1}/{len(train_loader)}',\n",
    "                f'  Train Loss: {total_loss / steps:.4f}',\n",
    "                f'  LR: {current_lr:.1e}', flush=True\n",
    "            )\n",
    "            total_loss = 0\n",
    "            steps = 0\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step()\n",
    "    after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(\"Epoch %d: adam lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            prompt_input_ids = batch['prompt_inputs'].to(device)\n",
    "            prompt_attention_mask = batch['prompt_attention_mask'].to(device)\n",
    "            response_a_input_ids = batch['response_a_inputs'].to(device)\n",
    "            response_a_attention_mask = batch['response_a_attention_mask'].to(device)\n",
    "            winner_a = batch['winner_model_a'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prompt_embeddings = model.embedding_model(prompt_input_ids, prompt_attention_mask).last_hidden_state\n",
    "                response_a_embeddings = model.embedding_model(response_a_input_ids, response_a_attention_mask).last_hidden_state\n",
    "\n",
    "            logits = model(prompt_embeddings, response_a_embeddings).squeeze(-1)\n",
    "            val_loss += criterion(logits, winner_a.float()).item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    \n",
    "    test = get_predictions(test, model, tokenizer, device)\n",
    "    get_accuracy(test)\n",
    "    \n",
    "    if val_loss < val_loss_threshold:\n",
    "        val_loss_threshold = val_loss\n",
    "        torch.save(model.state_dict(), f'model_a_{epoch+1}.pth')\n",
    "        print('saving model...')\n",
    "    \n",
    "    print('============================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModel.from_pretrained(\"allenai/longformer-base-4096\").to(device) #\n",
    "# tokenizer  = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class classifier_model(nn.Module):\n",
    "#     def __init__(self,model,tokenizer):\n",
    "#         super(classifier_model,self).__init__()\n",
    "#         self.model = model\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#     def forward(self,prompt,resp_a,resp_b):\n",
    "#         tokenized_prompt = self.tokenizer(prompt,truncation=True,padding='max_length',max_length = 1024,return_tensors='pt').to(device)\n",
    "#         tokenized_resp_a = self.tokenizer(resp_a,truncation=True,padding='max_length',max_length = 1024,return_tensors='pt').to(device)\n",
    "#         tokenized_resp_b = self.tokenizer(resp_b,truncation=True,padding='max_length',max_length = 1024,return_tensors='pt').to(device)\n",
    "        \n",
    "#         prompt_hs = self.model(**tokenized_prompt).last_hidden_state\n",
    "#         resp_a_hs = self.model(**tokenized_resp_a).last_hidden_state\n",
    "#         resp_b_hs = self.model(**tokenized_resp_b).last_hidden_state\n",
    "\n",
    "\n",
    "#         return prompt_hs,resp_a_hs,resp_b_hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = classifier_model(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p,a,b = classifier(\"hi, how are you?\",\"i'm good, what about you?\",\"capital of india is delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear = nn.Linear(in_features=768,out_features=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear(torch.cat((p,a),dim=1)).shape,linear(torch.cat((p,b),dim=1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
